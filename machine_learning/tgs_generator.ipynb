{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, cv2\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "import keras.backend as K\n",
    "from segmentation_models import Unet\n",
    "from segmentation_models.utils import set_trainable\n",
    "\n",
    "from tgs.encoders import BinEncoder\n",
    "from tgs.models import get_kernel_net, predict_lr\n",
    "from tgs.losses import bce_tversky, comp_metric, lovasz_loss, comp_metric_no_sigmoid\n",
    "from tgs.cyclical_learning_rate import CyclicSnapshot\n",
    "from tgs.preprocessing import extend_tb, reflect_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad(img):\n",
    "    img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)\n",
    "    img = reflect_lr(img, 11, 11)\n",
    "    img = extend_tb(img, 11, 11)                            \n",
    "    return img\n",
    "\n",
    "def train_generator(df):\n",
    "    \n",
    "    depth_range = np.expand_dims(np.linspace(-50, 50, 101), 1)\n",
    "    \n",
    "    while True:\n",
    "        shuffle_indices = np.arange(len(df))\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "        \n",
    "        for start in range(0, len(df), batch_size):\n",
    "            \n",
    "            end = min(start + batch_size, len(df))            \n",
    "            inds = shuffle_indices[start:end]\n",
    "            images = train_images[inds]\n",
    "            masks = train_masks[inds]\n",
    "            \n",
    "            n = len(inds)\n",
    "            batch_images = np.zeros((2*n, img_size, img_size, 3), dtype=np.float32)\n",
    "            batch_masks = np.zeros((2*n, 101, 101, 1), dtype=np.float32)\n",
    "            for i, (img, msk, z) in enumerate(zip(images, masks, df.z[inds])):\n",
    "                \n",
    "                batch_images[i,:,:,1] = resize_and_pad(img)\n",
    "                \n",
    "                dep = 0.001*np.repeat(z+depth_range, img.shape[0], 1)     \n",
    "                batch_images[i,:,:,1] = resize_and_pad(dep)\n",
    "            \n",
    "                ecdf_img = ECDF(img.flatten())(img)\n",
    "                batch_images[i,:,:,2] = resize_and_pad(ecdf_img)\n",
    "                                       \n",
    "                batch_images[n+i,:,:,:] = batch_images[i,:,::-1,:]\n",
    "                \n",
    "                batch_masks[i,:,:,0] = msk\n",
    "                batch_masks[n+i,:,:,:] =  batch_masks[i,:,::-1,:]\n",
    "                \n",
    "            yield batch_images, batch_masks\n",
    "            \n",
    "def valid_generator(df):\n",
    "    \n",
    "    depth_range = np.expand_dims(np.linspace(-50, 50, 101), 1)\n",
    "    \n",
    "    while True:\n",
    "        shuffle_indices = np.arange(len(df))\n",
    "        shuffle_indices = np.random.permutation(shuffle_indices)\n",
    "        \n",
    "        for start in range(0, len(df), batch_size):\n",
    "            \n",
    "            end = min(start + batch_size, len(df))            \n",
    "            inds = shuffle_indices[start:end]\n",
    "            images = train_images[inds]\n",
    "            masks = train_masks[inds]\n",
    "            \n",
    "            n = len(inds)\n",
    "            batch_images = np.zeros((n, img_size, img_size, 3), dtype=np.float32)\n",
    "            batch_masks = np.zeros((n, 101, 101, 1), dtype=np.float32)\n",
    "            for i, (img, msk, z) in enumerate(zip(images, masks, df.z[inds])):\n",
    "                \n",
    "                batch_images[i,:,:,1] = resize_and_pad(img)\n",
    "                \n",
    "                dep = 0.001*np.repeat(z+depth_range, img.shape[0], 1)     \n",
    "                batch_images[i,:,:,1] = resize_and_pad(dep)\n",
    "            \n",
    "                ecdf_img = ECDF(img.flatten())(img)\n",
    "                batch_images[i,:,:,2] = resize_and_pad(ecdf_img)\n",
    "                       \n",
    "                batch_masks[i,:,:,0] = msk\n",
    "                \n",
    "            yield batch_images, batch_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "MACHINE = 'matt'\n",
    "backbone = 'resnet50'\n",
    "version = 'qubvel-1c'\n",
    "\n",
    "n_folds = 10\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "decoder_filters = (64, 32, 16, 8, 4)\n",
    "cycle_len = 3\n",
    "    \n",
    "model_name = '_'.join([\n",
    "    f'{version}',\n",
    "    f'backbone-{backbone}',    \n",
    "    f'decoder-{\"-\".join(str(x) for x in decoder_filters)}',    \n",
    "    f'size-{img_size}',\n",
    "    f'cycle_len-{cycle_len}',\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "FILE_PATH = f'/home/{MACHINE}/Dropbox/Kaggle/tgs'\n",
    "train_df = pd.read_csv(f'{FILE_PATH}/train.csv', index_col='id', usecols=[0])\n",
    "depths_df = pd.read_csv(f'{FILE_PATH}/depths.csv', index_col='id')\n",
    "\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classes \n",
    "be = BinEncoder(n_bins=10)\n",
    "test_df['class'] = be.fit_transform(test_df.z.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(path, indexs):\n",
    "    images = np.zeros((len(indexs), 101, 101, 1), dtype=np.float32)\n",
    "    for i, idx in enumerate(indexs):\n",
    "        images[i,:,:,0] = cv2.imread(f'{path}/{idx}.png',0)/255\n",
    "    return images\n",
    "\n",
    "# load images\n",
    "IMAGE_PATH = f'/media/{MACHINE}/storage0/kaggle-tgs-data'\n",
    "train_images = load_images(f'{IMAGE_PATH}/test_images', test_df.index)\n",
    "train_masks = np.load(f'{FILE_PATH}/blends/kernelnet-4_alpha_0.45_mask-best_test.npy').astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def get_unet(final_activation, **kwargs):\n",
    "    base_model = Unet(**kwargs, activation='linear')\n",
    "    x = base_model.output\n",
    "    x = keras.layers.Cropping2D(cropping=[[11, 11], [11, 11]])(x)\n",
    "    x = keras.layers.AveragePooling2D()(x)\n",
    "    y = keras.layers.Activation(final_activation)(x)\n",
    "    model = keras.Model(inputs=base_model.input, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# initialize containers\n",
    "loss_list, metric_list, epoch_list, = [], [], []\n",
    "\n",
    "# cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "fold_generator = kfold.split(test_df, test_df['class'])\n",
    "train_idx, valid_idx = next(fold_generator)\n",
    "\n",
    "# get step size\n",
    "step_size = (cycle_len*len(train_idx))//(2*batch_size)\n",
    "\n",
    "# pretrain model decoder\n",
    "K.clear_session()\n",
    "model = get_unet(final_activation='sigmoid', \n",
    "                 backbone_name=backbone,\n",
    "                 input_shape=(img_size, img_size, 3),\n",
    "                 encoder_weights='imagenet11k-places365ch',\n",
    "                 freeze_encoder=False,\n",
    "                 decoder_filters=decoder_filters,\n",
    "                 decoder_use_batchnorm=True)\n",
    "model.compile('sgd', bce_tversky, [comp_metric])\n",
    "\n",
    "# create callbacks\n",
    "weights_file = f'{FILE_PATH}/weights/{model_name}_stage-1'\n",
    "callbacks_list = [CyclicSnapshot(\n",
    "    cycle_len=cycle_len,\n",
    "    base_lr=0.001, \n",
    "    max_lr=0.01,    \n",
    "    mode='triangular2',\n",
    "    step_size = step_size,\n",
    "    filename=weights_file)]      \n",
    "\n",
    "# initialize model\n",
    "model.fit_generator(generator=train_generator(test_df.iloc[train_idx]),\n",
    "                    steps_per_epoch=np.ceil(float(len(train_idx)) / float(batch_size)),\n",
    "                    epochs=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=valid_generator(test_df.iloc[valid_idx]),\n",
    "                    validation_steps=np.ceil(float(len(valid_idx)) / float(batch_size)))\n",
    "\n",
    "# get new model without activation\n",
    "K.clear_session()\n",
    "model = get_unet(final_activation='linear', \n",
    "                 backbone_name=backbone,\n",
    "                 input_shape=(img_size, img_size, 3),\n",
    "                 encoder_weights=None,\n",
    "                 freeze_encoder=False,\n",
    "                 decoder_filters=decoder_filters,\n",
    "                 decoder_use_batchnorm=True)\n",
    "\n",
    "model.load_weights(f'{FILE_PATH}/weights/{model_name}_stage-1-5.hdf5')\n",
    "model.compile('sgd', lovasz_loss, [comp_metric_no_sigmoid])\n",
    "\n",
    "# create callbacks\n",
    "weights_file = f'{FILE_PATH}/weights/{model_name}_stage-2'\n",
    "callbacks_list = [CyclicSnapshot(\n",
    "    cycle_len=cycle_len,\n",
    "    base_lr=0.0001, \n",
    "    max_lr=0.001,    \n",
    "    mode='triangular2',\n",
    "    step_size = step_size,\n",
    "    filename=weights_file)]      \n",
    "\n",
    "# fine tune\n",
    "model.fit_generator(generator=train_generator(test_df.iloc[train_idx]),\n",
    "                    steps_per_epoch=np.ceil(float(len(train_idx)) / float(batch_size)),\n",
    "                    epochs=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=valid_generator(test_df.iloc[valid_idx]),\n",
    "                    validation_steps=np.ceil(float(len(valid_idx)) / float(batch_size)))\n",
    "\n",
    "# evaluate predictions\n",
    "min_loss = np.min(model.history.history['val_loss'])\n",
    "max_metric = np.max(model.history.history['val_comp_metric_no_sigmoid'])\n",
    "best_epoch = np.argmax(model.history.history['val_comp_metric_no_sigmoid'])\n",
    "\n",
    "print(min_loss, max_metric, best_epoch)\n",
    "loss_list.append(min_loss)\n",
    "epoch_list.append(best_epoch)\n",
    "metric_list.append(max_metric)\n",
    "\n",
    "# log results\n",
    "log_file = f'{FILE_PATH}/logs/{model_name}_{np.mean(max_metric):0.5f}.csv'\n",
    "logger = pd.DataFrame({\n",
    "    'backbone': 'backbone',    \n",
    "    'decoder': \"-\".join(str(x) for x in decoder_filters),    \n",
    "    'size': img_size,\n",
    "    'cycle_len': cycle_len,\n",
    "    'batch_size': batch_size,\n",
    "    'mean_loss': np.mean(loss_list),\n",
    "    'mean_metric': np.mean(metric_list),\n",
    "    'best_epoch': np.median(epoch_list),},\n",
    "    index=[0])\n",
    "logger.to_csv(log_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle]",
   "language": "python",
   "name": "conda-env-kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
