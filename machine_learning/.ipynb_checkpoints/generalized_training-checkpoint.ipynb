{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = '/home/matt/repos/ohia.ai/data'\n",
    "\n",
    "model_name = 'mobilenetv2'\n",
    "\n",
    "training_type = 0\n",
    "# 0: finetune using pretrained on ImageNet\n",
    "# 1: pretrain on PlantNet\n",
    "# 2: finetune using pretrained on PlantNet\n",
    "\n",
    "seed = 1\n",
    "batch_size = 32\n",
    "\n",
    "filtered = False\n",
    "augmentation = False\n",
    "\n",
    "n_thread = 32\n",
    "gpu = 0\n",
    "\n",
    "save_model = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob, json\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(gpu)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ohia.encoders import FastLabelEncoder\n",
    "from ohia.utils import PlantNetGenerator, make_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_name, training_type, seed, batch_size, augmentation):\n",
    "    \"\"\" Get path for given model parameters\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of neural network architecture, one of\n",
    "          \"mobilenetv1\", \"mobilenetv2\", \"nasnetmobile\"\n",
    "        training_type: Integer encoding:\n",
    "            * if 0 fine-tune\n",
    "            * if 1 pretrain\n",
    "            * if 2 load pretrained then fine-tune.\n",
    "        seed: Random seed.\n",
    "        batch_size: Number of observations needed before updating weights.\n",
    "        augmentation: Boolean flag.  If true then perform data augmentation.\n",
    "    \"\"\"\n",
    "    model_path = f'{FILE_PATH}/models/'\n",
    "    \n",
    "    # fine-tuning\n",
    "    if training_type == 0:\n",
    "        model_path += f'finetuned_{model_name}'\n",
    "        model_path += '_with_augmention' if augmentation else ''\n",
    "        \n",
    "    # pretraining\n",
    "    elif training_type == 1:\n",
    "        model_path += f'pretrained_{model_name}'\n",
    "        model_path += '_with_augmention' if augmentation else ''\n",
    "        \n",
    "    # pretraining then fine-tuning\n",
    "    elif training_type == 2:\n",
    "        model_path += f'finetuned_{model_name}'\n",
    "        model_path += '_with_augmention' if augmentation else ''\n",
    "        model_path += '_using_plantnet_pretrained'\n",
    "    else :\n",
    "        raise ValueError('training_type must be either 0, 1, or 2.') \n",
    "            \n",
    "    model_path += f'_seed-{seed}_batch_size-{batch_size}'\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, n_classes, training_type, pretrain_file=None):\n",
    "    \"\"\" Get ohiaNet architecture\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of neural network architecture, one of\n",
    "          \"mobilenetv1\", \"mobilenetv2\", \"nasnetmobile\"\n",
    "        n_classes: Numeber of output units\n",
    "        training_type: Integer encoding:\n",
    "            * if 0 fine-tune\n",
    "            * if 1 pretrain\n",
    "            * if 2 load pretrained then fine-tune.\n",
    "            \n",
    "    Returns: Keras model.\n",
    "    \"\"\"        \n",
    "    from keras import layers, Model\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.applications.mobilenet import MobileNet\n",
    "    from keras.applications.mobilenetv2 import MobileNetV2\n",
    "    from keras.applications.nasnet import NASNetMobile\n",
    "    from ohia.metrics import top_1_accuracy, top_3_accuracy, top_5_accuracy\n",
    "\n",
    "    import keras.backend as K\n",
    "    K.clear_session()\n",
    "\n",
    "    # load pretrained ImageNet model\n",
    "    if model_name == 'mobilenetv1':\n",
    "        base_model = MobileNet(\n",
    "            input_shape=(224,224,3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        )        \n",
    "    elif model_name == 'mobilenetv2':\n",
    "        base_model = MobileNetV2(\n",
    "            input_shape=(224,224,3),\n",
    "            alpha=1.4,\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        )\n",
    "    elif model_name == 'nasnetmobile':\n",
    "        base_model = NASNetMobile(\n",
    "            input_shape=(224,224,3),\n",
    "            weights='imagenet',\n",
    "            include_top=False\n",
    "        )\n",
    "    else:\n",
    "        assert ValueError(\n",
    "            f'model_name parameter must be one of the following'\n",
    "            ' \"mobilenetv1\",'\n",
    "            ' \"mobilenetv2\",'\n",
    "            ' \"nasnetmobile\"'\n",
    "        )\n",
    "\n",
    "        \n",
    "    if training_type==0:\n",
    "        \n",
    "        # map ImageNet features to plants\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)        \n",
    "        \n",
    "        # freeze all but the last layer\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable = False\n",
    "\n",
    "    elif training_type==1:\n",
    "        \n",
    "        # map ImageNet features to plants\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
    "        model = Model(inputs=base_model.input, outputs=outputs)        \n",
    "                    \n",
    "    elif training_type==2:\n",
    "        \n",
    "        N_PLANTNET_CLASSES = 436\n",
    "            \n",
    "        # map ImageNet features to plants\n",
    "        x = base_model.output\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        outputs = layers.Dense(N_PLANTNET_CLASSES, activation='relu')(x)\n",
    "\n",
    "        # load the model\n",
    "        pretrained_model = Model(inputs=base_model.input, outputs=outputs)\n",
    "        pretrained_model.load_weights(pretrain_file)\n",
    "        \n",
    "        # add final layer\n",
    "        outputs = layers.Dense(n_classes, activation='softmax')(pretrained_model.output)\n",
    "        model = Model(inputs=pretrained_model.input, outputs=outputs)\n",
    "\n",
    "        # freeze all but the last two layers\n",
    "        if training_dataset == 'scraped':\n",
    "            for layer in model.layers[:-2]:\n",
    "                layer.trainable = False\n",
    "            \n",
    "    else :\n",
    "        raise ValueError('training_type must be either 0, 1, or 2.') \n",
    "                \n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[top_1_accuracy, top_3_accuracy, top_5_accuracy]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory for current expirement\n",
    "model_path = get_model_path(model_name, 1, seed, batch_size, augmentation)\n",
    "make_dir(model_path)\n",
    "\n",
    "# get pretrained file\n",
    "pretrain_path = get_model_path(model_name, 1, seed, batch_size, augmentation)\n",
    "pretrain_file = np.sort(glob.glob(f'{pretrain_path}/**.h5'))[-1]\n",
    "\n",
    "# get list of images and labels\n",
    "image_dir = 'plantnet_filtered'if training_type==2 else 'scraped_filtered'\n",
    "file_list = glob.glob(f'{FILE_PATH}/preprocessed_images/{image_dir}/**/*.jpg', recursive=True)\n",
    "full_label_list = [re.split('/', f)[-2] for f in file_list]\n",
    "\n",
    "# encode label names with ids\n",
    "fle = FastLabelEncoder()\n",
    "label_ids = fle.fit_transform(full_label_list)\n",
    "\n",
    "# save id2label map\n",
    "id2label = {int(fle.transform([label])):label for label in np.unique(full_label_list)}\n",
    "with open(f'{model_path}/plantnet_classes.json', 'w') as fp:\n",
    "    json.dump(id2label, fp)\n",
    "\n",
    "# split data\n",
    "train_files, valid_files, train_ids, valid_ids \\\n",
    "    = train_test_split(file_list, label_ids, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generators\n",
    "n_classes = len(np.unique(full_label_list))\n",
    "train_generator = PlantNetGenerator(\n",
    "    train_files, train_ids, n_classes,\n",
    "    batch_size=batch_size,\n",
    "    augment=augmentation\n",
    ")\n",
    "valid_generator = PlantNetGenerator(\n",
    "    valid_files, valid_ids, n_classes,\n",
    "    batch_size=batch_size,\n",
    "    augment=augmentation,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_top_3_accuracy',\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_top_3_accuracy',\n",
    "        factor=0.25,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        monitor='val_top_3_accuracy',\n",
    "        filepath=f'{model_path}/weights' + '_{epoch:02d}.h5',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='max',\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "592/592 [==============================] - 76s 128ms/step - loss: 1.8007 - top_1_accuracy: 0.5116 - top_3_accuracy: 0.6994 - top_5_accuracy: 0.7817 - val_loss: 1.4881 - val_top_1_accuracy: 0.5928 - val_top_3_accuracy: 0.7817 - val_top_5_accuracy: 0.8596\n",
      "Epoch 2/100\n",
      "592/592 [==============================] - 68s 114ms/step - loss: 1.1794 - top_1_accuracy: 0.6572 - top_3_accuracy: 0.8355 - top_5_accuracy: 0.8990 - val_loss: 1.3014 - val_top_1_accuracy: 0.6486 - val_top_3_accuracy: 0.8221 - val_top_5_accuracy: 0.8798\n",
      "Epoch 3/100\n",
      "592/592 [==============================] - 73s 124ms/step - loss: 1.0146 - top_1_accuracy: 0.7012 - top_3_accuracy: 0.8661 - top_5_accuracy: 0.9201 - val_loss: 1.2194 - val_top_1_accuracy: 0.6750 - val_top_3_accuracy: 0.8341 - val_top_5_accuracy: 0.8913\n",
      "Epoch 4/100\n",
      "592/592 [==============================] - 75s 127ms/step - loss: 0.8899 - top_1_accuracy: 0.7366 - top_3_accuracy: 0.8891 - top_5_accuracy: 0.9357 - val_loss: 1.1360 - val_top_1_accuracy: 0.6962 - val_top_3_accuracy: 0.8510 - val_top_5_accuracy: 0.9048\n",
      "Epoch 5/100\n",
      "592/592 [==============================] - 71s 120ms/step - loss: 0.7878 - top_1_accuracy: 0.7645 - top_3_accuracy: 0.9061 - top_5_accuracy: 0.9462 - val_loss: 1.2362 - val_top_1_accuracy: 0.6716 - val_top_3_accuracy: 0.8394 - val_top_5_accuracy: 0.9000\n",
      "Epoch 6/100\n",
      "592/592 [==============================] - 71s 120ms/step - loss: 0.7184 - top_1_accuracy: 0.7802 - top_3_accuracy: 0.9189 - top_5_accuracy: 0.9554 - val_loss: 1.1686 - val_top_1_accuracy: 0.6880 - val_top_3_accuracy: 0.8639 - val_top_5_accuracy: 0.9091\n",
      "Epoch 7/100\n",
      "592/592 [==============================] - 70s 119ms/step - loss: 0.6391 - top_1_accuracy: 0.8049 - top_3_accuracy: 0.9329 - top_5_accuracy: 0.9655 - val_loss: 1.2384 - val_top_1_accuracy: 0.6870 - val_top_3_accuracy: 0.8510 - val_top_5_accuracy: 0.9019\n",
      "Epoch 8/100\n",
      "592/592 [==============================] - 70s 119ms/step - loss: 0.5817 - top_1_accuracy: 0.8187 - top_3_accuracy: 0.9397 - top_5_accuracy: 0.9701 - val_loss: 1.2152 - val_top_1_accuracy: 0.6933 - val_top_3_accuracy: 0.8591 - val_top_5_accuracy: 0.9087\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/100\n",
      "592/592 [==============================] - 70s 118ms/step - loss: 0.4278 - top_1_accuracy: 0.8673 - top_3_accuracy: 0.9629 - top_5_accuracy: 0.9828 - val_loss: 1.1570 - val_top_1_accuracy: 0.7120 - val_top_3_accuracy: 0.8601 - val_top_5_accuracy: 0.9159\n",
      "Epoch 10/100\n",
      "592/592 [==============================] - 71s 120ms/step - loss: 0.4032 - top_1_accuracy: 0.8758 - top_3_accuracy: 0.9663 - top_5_accuracy: 0.9851 - val_loss: 1.1743 - val_top_1_accuracy: 0.7077 - val_top_3_accuracy: 0.8601 - val_top_5_accuracy: 0.9163\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 11/100\n",
      "592/592 [==============================] - 72s 122ms/step - loss: 0.3660 - top_1_accuracy: 0.8872 - top_3_accuracy: 0.9701 - top_5_accuracy: 0.9878 - val_loss: 1.1370 - val_top_1_accuracy: 0.7207 - val_top_3_accuracy: 0.8639 - val_top_5_accuracy: 0.9202\n",
      "Epoch 12/100\n",
      "592/592 [==============================] - 74s 125ms/step - loss: 0.3593 - top_1_accuracy: 0.8905 - top_3_accuracy: 0.9699 - top_5_accuracy: 0.9878 - val_loss: 1.1579 - val_top_1_accuracy: 0.7149 - val_top_3_accuracy: 0.8625 - val_top_5_accuracy: 0.9212\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 13/100\n",
      "592/592 [==============================] - 73s 123ms/step - loss: 0.3464 - top_1_accuracy: 0.8963 - top_3_accuracy: 0.9717 - top_5_accuracy: 0.9876 - val_loss: 1.1514 - val_top_1_accuracy: 0.7154 - val_top_3_accuracy: 0.8644 - val_top_5_accuracy: 0.9236\n",
      "Epoch 14/100\n",
      "592/592 [==============================] - 69s 117ms/step - loss: 0.3431 - top_1_accuracy: 0.8983 - top_3_accuracy: 0.9730 - top_5_accuracy: 0.9885 - val_loss: 1.1465 - val_top_1_accuracy: 0.7192 - val_top_3_accuracy: 0.8635 - val_top_5_accuracy: 0.9212\n",
      "Epoch 15/100\n",
      "592/592 [==============================] - 75s 126ms/step - loss: 0.3428 - top_1_accuracy: 0.8969 - top_3_accuracy: 0.9720 - top_5_accuracy: 0.9879 - val_loss: 1.1447 - val_top_1_accuracy: 0.7173 - val_top_3_accuracy: 0.8639 - val_top_5_accuracy: 0.9212\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 16/100\n",
      "592/592 [==============================] - 74s 125ms/step - loss: 0.3369 - top_1_accuracy: 0.8981 - top_3_accuracy: 0.9736 - top_5_accuracy: 0.9891 - val_loss: 1.1453 - val_top_1_accuracy: 0.7163 - val_top_3_accuracy: 0.8649 - val_top_5_accuracy: 0.9212\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 16/100\n",
      "Epoch 17/100\n",
      "592/592 [==============================] - 71s 120ms/step - loss: 0.3401 - top_1_accuracy: 0.8978 - top_3_accuracy: 0.9739 - top_5_accuracy: 0.9893 - val_loss: 1.1459 - val_top_1_accuracy: 0.7168 - val_top_3_accuracy: 0.8649 - val_top_5_accuracy: 0.9221\n",
      "Epoch 18/100\n",
      "592/592 [==============================] - 70s 119ms/step - loss: 0.3344 - top_1_accuracy: 0.8983 - top_3_accuracy: 0.9746 - top_5_accuracy: 0.9896 - val_loss: 1.1451 - val_top_1_accuracy: 0.7178 - val_top_3_accuracy: 0.8654 - val_top_5_accuracy: 0.9216\n",
      "Epoch 19/100\n",
      "592/592 [==============================] - 72s 122ms/step - loss: 0.3399 - top_1_accuracy: 0.9000 - top_3_accuracy: 0.9743 - top_5_accuracy: 0.9885 - val_loss: 1.1466 - val_top_1_accuracy: 0.7178 - val_top_3_accuracy: 0.8649 - val_top_5_accuracy: 0.9216\n",
      "Epoch 20/100\n",
      "592/592 [==============================] - 72s 121ms/step - loss: 0.3376 - top_1_accuracy: 0.8976 - top_3_accuracy: 0.9747 - top_5_accuracy: 0.9895 - val_loss: 1.1446 - val_top_1_accuracy: 0.7188 - val_top_3_accuracy: 0.8659 - val_top_5_accuracy: 0.9226\n",
      "Epoch 21/100\n",
      "592/592 [==============================] - 71s 119ms/step - loss: 0.3364 - top_1_accuracy: 0.8989 - top_3_accuracy: 0.9739 - top_5_accuracy: 0.9902 - val_loss: 1.1444 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8649 - val_top_5_accuracy: 0.9226\n",
      "Epoch 22/100\n",
      "592/592 [==============================] - 71s 119ms/step - loss: 0.3347 - top_1_accuracy: 0.9013 - top_3_accuracy: 0.9743 - top_5_accuracy: 0.9893 - val_loss: 1.1445 - val_top_1_accuracy: 0.7173 - val_top_3_accuracy: 0.8659 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 23/100\n",
      "592/592 [==============================] - 71s 120ms/step - loss: 0.3387 - top_1_accuracy: 0.8979 - top_3_accuracy: 0.9725 - top_5_accuracy: 0.9889 - val_loss: 1.1444 - val_top_1_accuracy: 0.7173 - val_top_3_accuracy: 0.8659 - val_top_5_accuracy: 0.9221\n",
      "Epoch 24/100\n",
      "592/592 [==============================] - 71s 120ms/step - loss: 0.3375 - top_1_accuracy: 0.8982 - top_3_accuracy: 0.9742 - top_5_accuracy: 0.9885 - val_loss: 1.1444 - val_top_1_accuracy: 0.7188 - val_top_3_accuracy: 0.8659 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 25/100\n",
      "592/592 [==============================] - 72s 121ms/step - loss: 0.3369 - top_1_accuracy: 0.8990 - top_3_accuracy: 0.9741 - top_5_accuracy: 0.9886 - val_loss: 1.1444 - val_top_1_accuracy: 0.7188 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 25/100\n",
      "Epoch 26/100\n",
      "592/592 [==============================] - 73s 124ms/step - loss: 0.3371 - top_1_accuracy: 0.8990 - top_3_accuracy: 0.9736 - top_5_accuracy: 0.9882 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8659 - val_top_5_accuracy: 0.9221\n",
      "Epoch 26/100\n",
      "Epoch 27/100\n",
      "592/592 [==============================] - 72s 121ms/step - loss: 0.3411 - top_1_accuracy: 0.8963 - top_3_accuracy: 0.9737 - top_5_accuracy: 0.9892 - val_loss: 1.1445 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 28/100\n",
      "592/592 [==============================] - 72s 121ms/step - loss: 0.3338 - top_1_accuracy: 0.8997 - top_3_accuracy: 0.9750 - top_5_accuracy: 0.9892 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "592/592 [==============================] - 74s 125ms/step - loss: 0.3348 - top_1_accuracy: 0.8985 - top_3_accuracy: 0.9727 - top_5_accuracy: 0.9887 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "Epoch 30/100\n",
      "592/592 [==============================] - 71s 119ms/step - loss: 0.3339 - top_1_accuracy: 0.9008 - top_3_accuracy: 0.9733 - top_5_accuracy: 0.9893 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "Epoch 31/100\n",
      "592/592 [==============================] - 71s 119ms/step - loss: 0.3353 - top_1_accuracy: 0.9002 - top_3_accuracy: 0.9737 - top_5_accuracy: 0.9895 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "Epoch 32/100\n",
      "592/592 [==============================] - 74s 125ms/step - loss: 0.3392 - top_1_accuracy: 0.8979 - top_3_accuracy: 0.9736 - top_5_accuracy: 0.9890 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "Epoch 33/100\n",
      "592/592 [==============================] - 69s 117ms/step - loss: 0.3400 - top_1_accuracy: 0.8986 - top_3_accuracy: 0.9733 - top_5_accuracy: 0.9889 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "Epoch 34/100\n",
      "592/592 [==============================] - 71s 120ms/step - loss: 0.3360 - top_1_accuracy: 0.9016 - top_3_accuracy: 0.9748 - top_5_accuracy: 0.9893 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "Epoch 35/100\n",
      "592/592 [==============================] - 72s 121ms/step - loss: 0.3376 - top_1_accuracy: 0.8988 - top_3_accuracy: 0.9720 - top_5_accuracy: 0.9892 - val_loss: 1.1446 - val_top_1_accuracy: 0.7183 - val_top_3_accuracy: 0.8663 - val_top_5_accuracy: 0.9221\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc770f05080>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "model = get_model(model_name, n_classes, training_type, pretrain_file)\n",
    "model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    callbacks=callbacks_list,\n",
    "    use_multiprocessing=True,\n",
    "    workers=n_thread,\n",
    "    epochs=100,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "results = pd.DataFrame(model.history.history)\n",
    "results.to_csv(f'{model_path}/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss              1.144416e+00\n",
      "val_top_1_accuracy    7.187500e-01\n",
      "val_top_3_accuracy    8.663462e-01\n",
      "val_top_5_accuracy    9.221154e-01\n",
      "loss                  3.369053e-01\n",
      "top_1_accuracy        8.990182e-01\n",
      "top_3_accuracy        9.740815e-01\n",
      "top_5_accuracy        9.885980e-01\n",
      "lr                    2.441406e-07\n",
      "Name: 24, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# print best results\n",
    "print(results.iloc[results.val_top_3_accuracy.values.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "if save_model:\n",
    "    import tensorflowjs as tfjs\n",
    "    model = get_model(model_name, n_classes)\n",
    "    best_weights = glob.glob(f'{model_path}/**.h5')\n",
    "    best_weights = np.sort(best_weights)[-1]\n",
    "    model.load_weights(best_weights)\n",
    "    tfjs.converters.save_keras_model(model, f'{model_path}/tfjs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
